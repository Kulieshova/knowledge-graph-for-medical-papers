{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7f94c98-cdbe-4b1a-8787-823dd648596c",
   "metadata": {},
   "source": [
    "# GPT Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb697bf-a6b8-4570-91ee-eb59dcc56ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 314 LLM outputs against 1412 ground truth statements...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating relationships:   0%|                         | 0/314 [00:02<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "import re\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import fitz\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key='')\n",
    "def read_pdf(pdf_file):\n",
    "    start=False\n",
    "    sentences=[]\n",
    "    start_idx=0\n",
    "    with fitz.open(pdf_file) as pdf_file:\n",
    "        for page_index, page in enumerate(pdf_file):\n",
    "            text = page.get_text(\"text\").lower()\n",
    "            text=text.split(\". \")\n",
    "            sentences.extend(text)\n",
    "                \n",
    "    return sentences\n",
    "def read_files(root_dir, hand):\n",
    "    \n",
    "    lines=[]\n",
    "    for files in os.listdir(root_dir):\n",
    "        if files[-4:] != '.pdf':\n",
    "            continue\n",
    "        sentences = read_pdf(f\"{root_dir}/{files}\")\n",
    "        lines.extend(sentences)\n",
    "\n",
    "    # read in hand annotations\n",
    "    for p in hand.iterrows():\n",
    "        rel = p[1]['rel']\n",
    "        subj = p[1]['subj']\n",
    "        obj = p[1]['obj']\n",
    "        out=f\"{subj} {rel} {obj}\" \n",
    "        lines.append(out)\n",
    "\n",
    "\n",
    "    return lines\n",
    "\n",
    "def extract_score(evaluation):\n",
    "    \"\"\"Extract numerical score from GPT evaluation\"\"\"\n",
    "    match = re.search(r'(?:Overall correctness score|Score|I would score the LLM output a)[:\\s]*([0-1](?:\\.\\d+)?)', evaluation, re.IGNORECASE)\n",
    "    return float(match.group(1)) if match else -1\n",
    "\n",
    "def is_relevant(ground_truth, llm_output):\n",
    "    \"\"\"Check if LLM output is relevant to ground truth\"\"\"\n",
    "    gt_terms = set(ground_truth.lower().split())\n",
    "    llm_terms = set(llm_output.lower().split())\n",
    "    return len(gt_terms.intersection(llm_terms)) > 1\n",
    "\n",
    "def get_critic_gpt_evaluation(llm_text, gt_text):\n",
    "    \"\"\"Get evaluation from GPT for a single comparison\"\"\"\n",
    "    try:\n",
    "        prompt = f\"\"\"\n",
    "        Compare these statements:\n",
    "        Ground Truth: {gt_text}\n",
    "        LLM Output: {llm_text}\n",
    "        \n",
    "        Score from 0 to 1 for overall correctness (1 being highly correct). Format: 'Score: X.XX'\n",
    "        Brief explanation (one sentence).\n",
    "        \"\"\"\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=150\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during evaluation: {e}\")\n",
    "        return None\n",
    "\n",
    "def evaluate_single_output(llm_row, ground_truth):\n",
    "    \"\"\"Evaluate a single LLM output against all ground truth entries\"\"\"\n",
    "    llm_text = f\"{llm_row['subj']} {llm_row['rel']} {llm_row['obj']}\"\n",
    "    best_score = -1\n",
    "    best_evaluation = None\n",
    "    best_gt_row = None\n",
    "    \n",
    "    for gt in ground_truth:\n",
    "        if not is_relevant(gt, llm_text):\n",
    "            continue\n",
    "            \n",
    "        evaluation = get_critic_gpt_evaluation(llm_text, gt)\n",
    "        \n",
    "        if evaluation:\n",
    "            score = extract_score(evaluation)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_evaluation = evaluation\n",
    "                best_gt_row = gt\n",
    "    \n",
    "    return {\n",
    "        'llm_output': llm_text,\n",
    "        'best_matching_ground_truth': best_gt_row,\n",
    "        'best_evaluation': best_evaluation,\n",
    "        'best_score': best_score\n",
    "    }\n",
    "\n",
    "def evaluate_all_outputs(llm_output_df, ground_truth_df, max_workers=10):\n",
    "    \"\"\"Process all LLM outputs in parallel\"\"\"\n",
    "    all_evaluations = []\n",
    "    total = len(llm_output_df)\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Create future tasks\n",
    "        future_to_row = {\n",
    "            executor.submit(evaluate_single_output, row, ground_truth_df): i \n",
    "            for i, (_, row) in enumerate(llm_output_df.iterrows())\n",
    "        }\n",
    "        \n",
    "        # Process results with progress bar\n",
    "        with tqdm(total=total, desc=\"Evaluating relationships\") as pbar:\n",
    "            for future in concurrent.futures.as_completed(future_to_row):\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    if result['best_score'] != -1:\n",
    "                        all_evaluations.append(result)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing row: {e}\")\n",
    "                pbar.update(1)\n",
    "    \n",
    "    return all_evaluations\n",
    "\n",
    "# Load data\n",
    "final_evals = []\n",
    "hand = pd.read_csv(\"../Results/ground_truth.csv\")\n",
    "ground_truth = read_files(\"../Docs\",hand)\n",
    "rel_docs = [\"Temperature0point2.csv\", \n",
    "            \"Temperature1_WithExamples.csv\", \"Temperature1_WithoutExamples.csv\", \n",
    "            \"Temperature1_WithoutExamples.csv\", 'NewRels_Skip1_Temperature1.csv']\n",
    "\n",
    "for doc in rel_docs[-1:]:\n",
    "    doc_ = f'../Results/{doc}'\n",
    "    llm_output = pd.read_csv(doc_)\n",
    "    print(f\"Processing {len(llm_output)} LLM outputs against {len(ground_truth)} ground truth statements...\")\n",
    "\n",
    "    # Run evaluation\n",
    "    evaluations = evaluate_all_outputs(\n",
    "        llm_output_df=llm_output,\n",
    "        ground_truth_df=ground_truth,\n",
    "        max_workers=10\n",
    "    )\n",
    "\n",
    "    # Calculate average score\n",
    "    valid_scores = [eval['best_score'] for eval in evaluations if eval['best_score'] != -1]\n",
    "    average_score = np.mean(valid_scores) if valid_scores else 0\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nOverall Average Score {doc_}: {average_score:.2f}\")\n",
    "    print(f\"Processed {len(evaluations)} evaluations\")\n",
    "    # Save results to DataFrame\n",
    "    final_evals.append(average_score)\n",
    "    results_df = pd.DataFrame(evaluations)\n",
    "    print(\"\\nSummary DataFrame:\")\n",
    "    print(results_df.head())\n",
    "    \n",
    "    # Save results\n",
    "    results_df.to_csv(f'GPT_critic_eval_for_{doc}', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd5487bc-b2b9-47b0-90b5-92501f0656f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for NewRels_Skip2_cummulative.csv: 0.72\n",
      "Accuracy for NewRels_Skip2_increments.csv: 0.67\n",
      "Accuracy for NewRels_Skip3_cummulative.csv: 0.72\n",
      "Accuracy for NewRels_Skip3_increments.csv: 0.68\n",
      "Accuracy for NewRels_Skip4_increments.csv: 0.65\n",
      "Accuracy for Temperature0point2.csv: 0.76\n",
      "Accuracy for Temperature1_WithExamples.csv: 0.74\n",
      "Accuracy for Temperature1_WithoutExamples_cleaned.csv: 0.76\n",
      "Accuracy for Temperature1_WithoutExamples.csv: 0.76\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(final_evals)):\n",
    "    print(f\"Accuracy for {rel_docs[i]}: {final_evals[i]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6894403a-3e94-4c8f-8ba1-85795c350a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(pdf_file):\n",
    "    start=False\n",
    "    sentences=[]\n",
    "    start_idx=0\n",
    "    with fitz.open(pdf_file) as pdf_file:\n",
    "        for page_index, page in enumerate(pdf_file):\n",
    "            text = page.get_text(\"text\").lower()\n",
    "            text=text.split(\". \")\n",
    "            for sub in text:\n",
    "                if 'abstract' in sub or 'intro' in sub:\n",
    "                    start=True\n",
    "                    if 'abstract' in sub:\n",
    "                        sub.index('abstract')\n",
    "                    else:\n",
    "                        sub.index('intro')\n",
    "                        \n",
    "                if start:\n",
    "                    sentences.append(sub)\n",
    "                \n",
    "    return sentences\n",
    "def read_files(root_dir, hand):\n",
    "    \n",
    "    lines=[]\n",
    "    for files in os.listdir(root_dir):\n",
    "        if files[-4:] != '.pdf':\n",
    "            continue\n",
    "        sentences = read_pdf(f\"{root_dir}/{files}\")\n",
    "        lines.extend(sentences)\n",
    "\n",
    "    # read in hand annotations\n",
    "    for p in hand.iterrows():\n",
    "        rel = p[1]['rel']\n",
    "        subj = p[1]['subj']\n",
    "        obj = p[1]['obj']\n",
    "        out=f\"{subj} {rel} {obj}\" \n",
    "        lines.append(out)\n",
    "\n",
    "\n",
    "    return lines\n",
    "hand = pd.read_csv(\"../Results/ground_truth.csv\")\n",
    "ground_truth = read_files(\"../Docs\",hand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e2d968-0aa2-4b7c-8565-11d192e1a0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(final_evals)):\n",
    "    print(f\"Accuracy for {rel_docs[i]}: {final_evals[i]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1b8132-8128-4529-b0f0-a1bb66586a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Overall Average Score ../Results/NewRels_Skip2_increments.csv: 0.89\n",
    "Overall Average Score ../Results/NewRels_Skip3_increments.csv: 0.86\n",
    "Overall Average Score ../Results/NewRels_Skip4_increments.csv: 0.87\n",
    "Overall Average Score ../Results/Temperature1_WithoutExamples.csv: 0.90\n",
    "Overall Average Score ../Results/Temperature0point2.csv: 0.91"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
