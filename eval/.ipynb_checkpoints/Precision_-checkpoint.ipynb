{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd4f32da-0be6-4691-94f4-1ec6dfdf75c9",
   "metadata": {},
   "source": [
    "# Precision: Calculating how many of the extracted relationships are valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "678282a3-737a-42a8-b656-b030f202e980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.12/site-packages (3.8.1)\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (4.66.4)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Installing collected packages: nltk\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.8.1\n",
      "    Uninstalling nltk-3.8.1:\n",
      "      Successfully uninstalled nltk-3.8.1\n",
      "Successfully installed nltk-3.9.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36666480-cfc6-42ba-8363-b717074f0033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz,os\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download(\"punkt\")\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df503792-98b1-4152-8e12-a4b4b73aae2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained sentence transformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Initialize Python porter stemmer\n",
    "ps = PorterStemmer()\n",
    "def lemmatize(sent):\n",
    "    '''\n",
    "        Lemmatizes word in sentence\n",
    "        args: \n",
    "            sent: string of words\n",
    "        return:\n",
    "            list of lemmatized words\n",
    "    '''\n",
    "    return [ps.stem(word) for word in sent.split()]\n",
    "    \n",
    "# Reading the pdf and hand annotation files\n",
    "def read_pdf(pdf_file):\n",
    "    '''\n",
    "        Reads text from pdf\n",
    "        args: \n",
    "            pdf_file: string representing name of file to read\n",
    "        return:\n",
    "            list of words from pdf\n",
    "    '''\n",
    "    start=False\n",
    "    sentences=[]\n",
    "    start_idx=0\n",
    "    with fitz.open(pdf_file) as pdf_file:\n",
    "        for page_index, page in enumerate(pdf_file):\n",
    "            text = page.get_text(\"text\").lower()\n",
    "            text=text.split(\". \")\n",
    "            sentences.extend(text)\n",
    "                \n",
    "    return sentences\n",
    "\n",
    "\n",
    "def read_files(root_dir, hand):\n",
    "    '''\n",
    "        Reads all files in a directory and all text in csv\n",
    "        args:\n",
    "            root_dir: directory from which to read files \n",
    "            hand: csv with hand annotations\n",
    "        return:\n",
    "            list of text from hand and documents in root_dir\n",
    "    '''\n",
    "    lines=[]\n",
    "    for files in os.listdir(root_dir):\n",
    "        if files[-4:] != '.pdf':\n",
    "            continue\n",
    "        sentences = read_pdf(f\"{root_dir}/{files}\")\n",
    "        lines.extend(sentences)\n",
    "\n",
    "    # read in hand annotations\n",
    "    for p in hand.iterrows():\n",
    "        rel = p[1]['rel']\n",
    "        subj = p[1]['subj']\n",
    "        obj = p[1]['obj']\n",
    "        out=f\"{subj} {rel} {obj}\" \n",
    "        lines.append(out)\n",
    "\n",
    "    return lines\n",
    "\n",
    "#computing cosine similarity\n",
    "def vec(sentences):\n",
    "    '''\n",
    "        Computes cosine similarity between 2 sentences\n",
    "        args:\n",
    "            sentences: list of 2 sentences\n",
    "        return:\n",
    "            similarity score\n",
    "    '''\n",
    "    # Encode sentences\n",
    "    embeddings = model.encode([sentences[0], sentences[1]])\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    similarity = util.cos_sim(embeddings[0], embeddings[1])\n",
    "    return similarity.item() # Value close to 1 indicates high similarity\n",
    "    \n",
    "#finding if the target string (relation triplet) is in the src (pdf + hand annotation)\n",
    "def find(target, src):\n",
    "    '''\n",
    "        Finds target sentence in src\n",
    "        args:\n",
    "            target: sentence to search for\n",
    "            src: list to search through\n",
    "\n",
    "        return:\n",
    "            boolean representing if text was found and sentence most closely aligning with target\n",
    "    '''\n",
    "    found=False\n",
    "    matching_sentence=\"\"\n",
    " \n",
    "    for idx,sentence in enumerate(src):\n",
    "        pred=\" \".join(lemmatize(target))\n",
    "        test=\" \".join(lemmatize(sentence))\n",
    "        cos = vec([pred,test])\n",
    "        if pred in test or cos > 0.7:\n",
    "            if cos >0.65 and cos < 0.7:\n",
    "                print(f\"Got a match for {pred }: {sentence}\")\n",
    "            elif cos <=0.65:\n",
    "                print(f\"Closest match to {pred} was {test}\")\n",
    "            found=True\n",
    "            st_idx=idx\n",
    "            matching_sentence=sentence\n",
    "            return found, matching_sentence, \n",
    "            \n",
    "    return found, matching_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666f4386-c9ef-4cc3-9bed-c991284eadef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(ground_truth_file, pred_files_dir):\n",
    "    \"\"\"\n",
    "        Goes through prediction files and finds how many of the extracted relationships are found in the ground truth\n",
    "        args:\n",
    "            ground_truth_file: file containing results to compare predictions to (combination of manual annotations and src documents)\n",
    "            pred_files: LLM extractions\n",
    "        return:\n",
    "            precision per file\n",
    "    \"\"\"\n",
    "    ground_truth = pd.read_csv(f\"../Results/{ground_truth_file}\")\n",
    "    pred_files = os.listdir(pred_files_dir)\n",
    "                            \n",
    "    sentences = read_files(\"../Docs\",ground_truth)\n",
    "    \n",
    "    \n",
    "    for pred_file in pred_files\n",
    "        pred_file = f\"../Results/{pred_file}\"\n",
    "        preds = pd.read_csv(pred_file) \n",
    "        score = 0\n",
    "        for p in preds.iterrows():\n",
    "            ref = p[1]['ref']\n",
    "            rel = p[1]['rel']\n",
    "            subj = p[1]['subj']\n",
    "            obj = p[1]['obj']\n",
    "            out=f\"{subj} {rel} {obj}\"\n",
    "            \n",
    "            found, match = find(out, sentences)\n",
    "            if found:\n",
    "                score +=1\n",
    "            else:\n",
    "                print(\"Couldn't find a match for  \", out)\n",
    "        print(f\"Precision for {pred_file} is {score/len(preds)}\")\n",
    "ground_truth_file = \"ground_truth.csv\"\n",
    "pred_files_dir = \"../Results\"\n",
    "main(ground_truth_file, pred_files_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193f5b04-e33e-49ba-868f-6bc211825f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "Precision for ../NewRels_Skip3_PassingInIncrements.csv : 0.8181818181818182\n",
    "Precision for ../NewRels_Skip4_increments.csv is 0.6785714285714286\n",
    "Precision for ../Results/NewRels_Skip3_cummulative.csv is 0.8584905660377359\n",
    "Precision for ../Results/NewRels_Skip2_cummulative.csv is 0.901840490797546\n",
    "Precision for ../Results/NewRels_Skip2_increments.csv is 0.7616279069767442\n",
    "Precision for ../Results/Temperature0point2.csv is 0.7981651376146789"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
